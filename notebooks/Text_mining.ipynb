{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:10809\"\n",
    "from ltp import LTP\n",
    "ltp = LTP(\"LTP/small\")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import emoji  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def parse_date_with_multiple_formats(date_str):\n",
    "    formats = ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M']  # 可以添加更多的格式\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return pd.NaT  # 如果所有格式都不匹配，返回 NaT\n",
    "\n",
    "# 假设 all_comments['date'] 是你需要解析的日期字符串列\n",
    "# all_comments['date'] = all_comments['date'].apply(parse_date_with_multiple_formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intergrate our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mainstream media list\n",
    "with open('官方媒体/官媒清单.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "    data = data.split('\\n')\n",
    "m_media = [item for item in data if item != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the comments dataset\n",
    "topic_comments = pd.read_csv('./data/新冠topic_comments_用户数据/新冠topic_comments.csv')\n",
    "\n",
    "# Define the types of users according to their verification type\n",
    "# '1' and '3' represent mainstream media, '0', '200', '220' denote self-media, and '-1' represents ordinary people\n",
    "topic_comments['type'] = 'None'  # Initialize the column with 'None'\n",
    "topic_comments.loc[topic_comments['comment_user_verify_type'] == 1, 'type'] = 'm_media'\n",
    "topic_comments.loc[topic_comments['comment_user_verify_type'] == 3, 'type'] = 'm_media'\n",
    "topic_comments.loc[topic_comments['comment_user_verify_type'].isin([0, 200, 220]), 'type'] = 'w_media'\n",
    "topic_comments.loc[topic_comments['comment_user_verify_type'] == -1, 'type'] = 'o_people'\n",
    "\n",
    "# Overwrite the type if the user name is in the predefined list of mainstream media\n",
    "topic_comments.loc[topic_comments['comment_user_name'].isin(m_media), 'type'] = 'm_media'\n",
    "\n",
    "# Extract and clean the list of unique user names for ordinary people\n",
    "people_name = topic_comments[topic_comments['type'] == 'o_people']['comment_user_name'].unique().tolist()\n",
    "people_name = [name for name in people_name if name]\n",
    "\n",
    "# Extract and clean the list of unique user names for self-media\n",
    "we_media_name = topic_comments[topic_comments['type'] == 'w_media']['comment_user_name'].unique().tolist()\n",
    "we_media_name = [name for name in we_media_name if name]\n",
    "\n",
    "# Extract and clean the list of unique user names for mainstream media\n",
    "m_media_name = topic_comments[topic_comments['type'] == 'm_media']['comment_user_name'].unique().tolist()\n",
    "m_media_name = [name for name in m_media_name if name]\n",
    "\n",
    "# Combine the mainstream media names with a predefined list of mainstream media\n",
    "m_media_name = list(set(m_media_name + m_media))\n",
    "\n",
    "# Select a sample of relevant columns for the analysis\n",
    "topic_comments_sample = topic_comments[['comment_user_name', 'comment_time', 'comment_content', 'type']]\n",
    "topic_comments_sample.columns = ['user_name', 'date', 'content', 'type']\n",
    "\n",
    "# Display the first few rows of the sample data\n",
    "topic_comments_sample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the second dataset\n",
    "another_comments = pd.read_excel('./output/data.xlsx')\n",
    "\n",
    "# Initialize the 'type' column\n",
    "another_comments['type'] = 'None'\n",
    "\n",
    "# Classify users into 'o_people' if they are not verified and 'w_media' if they are verified\n",
    "another_comments.loc[another_comments['type'] == '没有认证', 'type'] = 'o_people'\n",
    "another_comments.loc[another_comments['type'] != '没有认证', 'type'] = 'w_media'\n",
    "\n",
    "# If the user name is in the predefined list of mainstream media, classify as 'm_media'\n",
    "another_comments.loc[another_comments['user_name'].isin(m_media), 'type'] = 'm_media'\n",
    "\n",
    "# Extract and clean the list of unique user names for ordinary people\n",
    "another_people_name = another_comments[another_comments['type'] == 'o_people']['user_name'].unique().tolist()\n",
    "another_people_name = [name for name in another_people_name if name]\n",
    "\n",
    "# Extract and clean the list of unique user names for self-media\n",
    "another_we_media_name = another_comments[another_comments['type'] == 'w_media']['user_name'].unique().tolist()\n",
    "another_we_media_name = [name for name in another_we_media_name if name]\n",
    "\n",
    "# Extract and clean the list of unique user names for mainstream media\n",
    "another_m_media_name = another_comments[another_comments['type'] == 'm_media']['user_name'].unique().tolist()\n",
    "another_m_media_name = [name for name in another_m_media_name if name]\n",
    "\n",
    "# Merge the user names from both datasets to form a complete list\n",
    "all_people_name = list(set(people_name + another_people_name))\n",
    "all_we_media_name = list(set(we_media_name + another_we_media_name))\n",
    "all_m_media_name = list(set(m_media_name + another_m_media_name))\n",
    "\n",
    "# Select relevant columns for analysis and rename them\n",
    "another_comments_sample = another_comments[['user_name', 'publish_time', 'content', 'type']]\n",
    "another_comments_sample.columns = ['user_name', 'date', 'content', 'type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the third dataset\n",
    "repost_comments = pd.read_csv('./data/新冠topic_reposts_关系数据/新冠topic_reposts.csv')\n",
    "\n",
    "# Initialize the 'type' column to 'None'\n",
    "repost_comments['type'] = 'None'\n",
    "\n",
    "# Assign 'o_people' to user types if the user name is found in the list of all people names\n",
    "repost_comments.loc[repost_comments['user_name'].isin(all_people_name), 'type'] = 'o_people'\n",
    "\n",
    "# Assign 'w_media' to user types if the user name is found in the list of all self-media names\n",
    "repost_comments.loc[repost_comments['user_name'].isin(all_we_media_name), 'type'] = 'w_media'\n",
    "\n",
    "# Assign 'm_media' to user types if the user name is found in the list of all mainstream media names\n",
    "repost_comments.loc[repost_comments['user_name'].isin(all_m_media_name), 'type'] = 'm_media'\n",
    "\n",
    "# Count the number of occurrences for each user type\n",
    "repost_comments['type'].value_counts()\n",
    "\n",
    "# Filter out comments that have not been categorized into a type\n",
    "repost_comments = repost_comments[repost_comments['type'] != 'None']\n",
    "\n",
    "# Select relevant columns for further analysis and rename them\n",
    "repost_comments_sample = repost_comments[['user_name', 'publish_time', 'content', 'type']]\n",
    "repost_comments_sample.columns = ['user_name', 'date', 'content', 'type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all datasets\n",
    "all_comments = pd.concat([topic_comments_sample, another_comments_sample, repost_comments_sample], axis=0)\n",
    "all_comments.drop_duplicates(subset=['user_name', 'date', 'content', 'type'], inplace=True)\n",
    "all_comments['date'] = all_comments['date'].apply(parse_date_with_multiple_formats)\n",
    "all_comments['date'] = all_comments['date'].dt.date\n",
    "all_comments['date'] = pd.to_datetime(all_comments['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that Times New Roman is used as the font for all plot text\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 12  # You can adjust the base font size here if needed\n",
    "\n",
    "# Assuming 'forwarding' DataFrame and 'date' column are already defined\n",
    "# Create a 'year_month' column for grouping by year and month\n",
    "all_comments['year_month'] = all_comments['date'].dt.to_period('M')\n",
    "\n",
    "# Calculate the count of forwarding actions per month\n",
    "monthly_count = all_comments.groupby('year_month').size()\n",
    "\n",
    "# Start plotting\n",
    "plt.figure(figsize=(12, 8))  # Set the figure size for better detail\n",
    "\n",
    "# Plot data\n",
    "plt.plot(monthly_count.index.to_timestamp(), monthly_count.values, color='royalblue', marker='o', linestyle='-')\n",
    "\n",
    "# Improve the formatting of the x-axis to handle dates\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=3))  # Adjust interval for less crowded x-axis\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate the x-axis labels for better readability\n",
    "\n",
    "# Adding a title and labels with increased font sizes for readability\n",
    "plt.title('Monthly Post Count Distribution', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=14)\n",
    "plt.ylabel('Post Count', fontsize=14)\n",
    "\n",
    "# Optionally, set the x-axis to only have a maximum number of date ticks\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(6))\n",
    "\n",
    "# Remove grid lines for a cleaner look\n",
    "plt.grid(False)\n",
    "\n",
    "plt.savefig('./graph/post_count_distribution.png', format='png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "# Enhancing visual style by removing top and right borders\n",
    "sns.despine()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dates_count = all_comments['date'].isna().sum()\n",
    "print(f\"Number of missing or NaT dates: {missing_dates_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_topic = all_comments[all_comments['type'] == 'w_media']\n",
    "w_topic.to_csv('output/risk_media/w_content_info.csv', index=False)\n",
    "o_topic = all_comments[all_comments['type'] == 'o_people']\n",
    "o_topic.to_csv('output/sentiment/o_content_info.csv', index=False)\n",
    "m_topic = all_comments[all_comments['type'] == 'm_media']\n",
    "m_topic.to_csv('output/risk_media/m_content_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_index = []\n",
    "\n",
    "for index, row in all_comments.iterrows():\n",
    "    content = row['content']\n",
    "    if any(media in str(content) for media in m_media):\n",
    "        m_index.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_topic_add = all_comments.loc[m_index]\n",
    "m_topic_all = pd.concat([m_topic, m_topic_add], axis=0)\n",
    "m_topic_all.drop_duplicates(subset=['user_name', 'date', 'content', 'type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_topic_all.to_csv('output/risk_media/m_content_info_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_topic = topic_comments_sample[topic_comments_sample['type'] == 'w_media'][['comment_user_name', 'comment_content']]\n",
    "# w_topic.columns = ['user_name', 'content']\n",
    "# w_topic.to_excel('output/risk_media/w_content_info.xlsx')\n",
    "# o_topic = topic_comments_sample[topic_comments_sample['type'] == 'o_people'][['comment_user_name', 'comment_content']]\n",
    "# o_topic.columns = ['user_name', 'content']\n",
    "# o_topic.to_excel('output/sentiment/o_content_info.xlsx')\n",
    "# m_topic = topic_comments_sample[topic_comments_sample['type'] == 'm_media'][['comment_user_name', 'comment_content']]\n",
    "# m_topic.columns = ['user_name', 'content']\n",
    "# m_topic.to_excel('output/risk_media/m_content_info.xlsx')\n",
    "# m_comment_info = pd.read_excel('output/risk_media/m_content_info.xlsx')\n",
    "# pd.concat([m_topic, m_comment_info], axis=0).to_excel('output/risk_media/m_content_info.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments = pd.read_excel('./output/data.xlsx')\n",
    "# comments['date'] = pd.to_datetime(comments['publish_time'])\n",
    "# comments['date'] = comments['date'].dt.date\n",
    "# comments['date'] = pd.to_datetime(comments['date'])\n",
    "# comments_sample = comments[('2022-12-01' <= comments['date'])&(comments['date'] <= '2022-12-10')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the sentiment lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read lines, remove newlines and empty lines\n",
    "def read_and_clean(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        cleaned_lines = [line.strip() for line in lines if line.strip()]\n",
    "    return cleaned_lines\n",
    "\n",
    "# Read and clean lines from the 'high_sentiment.txt' file\n",
    "words_high = read_and_clean('./output/high_sentiment.txt')\n",
    "\n",
    "# Read and clean lines from the 'low_sentiment.txt' file\n",
    "words_low = read_and_clean('./output/low_sentiment.txt')\n",
    "\n",
    "# Read and clean lines from the 'middle_sentiment.txt' file\n",
    "words_middle = read_and_clean('./output/middle_sentiment.txt')\n",
    "\n",
    "# Read and clean lines from the 'sentiment_word.txt' file\n",
    "words_ini = read_and_clean('./output/sentiment_word.txt')\n",
    "\n",
    "# Combine words from all sentiment levels and remove duplicates\n",
    "words_combined = list(set(words_high + words_low + words_middle))\n",
    "\n",
    "# Add the combined list of words to the LTP for sentiment analysis\n",
    "ltp.add_words(words_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 自定义词表\n",
    "# ltp.add_word(\"汤姆去\", freq=2)\n",
    "# ltp.add_words([\"外套\", \"外衣\"], freq=2)\n",
    "\n",
    "# #  分词 cws、词性 pos、命名实体标注 ner、语义角色标注 srl、依存句法分析 dep、语义依存分析树 sdp、语义依存分析图 sdpg\n",
    "# output = ltp.pipeline([\"还是那句话，不管有没有后遗症，没病不要去得病，自己防护好，清洁做到位，不要去恐慌\"], tasks=[\"cws\", \"pos\"])\n",
    "# # 使用字典格式作为返回结果\n",
    "# print(output.cws)  # print(output[0]) / print(output['cws']) # 也可以使用下标访问\n",
    "# print(output.pos)\n",
    "# print(output.sdp)\n",
    "# 使用感知机算法实现的分词、词性和命名实体识别，速度比较快，但是精度略低\n",
    "# ltp_legacy = LTP(\"LTP/legacy\")\n",
    "# # cws, pos, ner = ltp.pipeline([\"他叫汤姆去拿外衣。\"], tasks=[\"cws\", \"ner\"]).to_tuple() # error: NER 需要 词性标注任务的结果\n",
    "# cws, pos, ner = ltp.pipeline([\"他叫汤姆去拿外衣。\"], tasks=[\"cws\", \"pos\", \"ner\"]).to_tuple()  # to tuple 可以自动转换为元组格式\n",
    "# # 使用元组格式作为返回结果\n",
    "# print(cws, pos, ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入一些语料\n",
    "# with open('./output/people_content_sample.txt', 'r', encoding='utf-8') as f:\n",
    "#     lines = f.readlines()\n",
    "#     lines = [line.strip() for line in lines]\n",
    "#     yuliao= [line for line in lines if line]\n",
    "# comments = pd.read_excel('./output/data.xlsx')\n",
    "# comments['date'] = pd.to_datetime(comments['publish_time'])\n",
    "# comments['date'] = comments['date'].dt.date\n",
    "# comments['date'] = pd.to_datetime(comments['date'])\n",
    "# comments_sample = comments[('2022-12-01' <= comments['date'])&(comments['date'] <= '2022-12-10')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# # 假定yuliao是您的文本列表，这里用示例文本代替\n",
    "# texts = yuliao\n",
    "\n",
    "# # 执行pipeline，获取分词和词性标注结果\n",
    "# segmented_texts = []  # 用于保存处理后的文本，以便计算TF-IDF\n",
    "# for text in texts:\n",
    "#     output = ltp.pipeline([text], tasks=[\"cws\", \"pos\"])\n",
    "#     words = output.cws[0]\n",
    "#     pos_tags = output.pos[0]\n",
    "\n",
    "#     # 根据词性选择关键词（这里以名词、动词、形容词、人名、组织名和成语为例）\n",
    "#     selected_words = [word for word, pos in zip(words, pos_tags) if pos in ['n', 'v', 'a', 'nh', 'ni', 'i']]\n",
    "#     segmented_texts.append(\" \".join(selected_words))\n",
    "\n",
    "# # 计算TF-IDF值\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(segmented_texts)\n",
    "\n",
    "# # 获取词汇和对应的TF-IDF值，然后打印排名前三的词汇\n",
    "# feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "# key_words = []\n",
    "# for i, text in enumerate(texts):\n",
    "#     print(f\"Text {i+1}: {text}\")\n",
    "#     tfidf_scores = tfidf_matrix.toarray()[i]\n",
    "#     word_scores = list(zip(feature_names, tfidf_scores))\n",
    "#     # 过滤掉TF-IDF值为0的词汇，然后根据TF-IDF值降序排序\n",
    "#     filtered_word_scores = [(word, score) for word, score in word_scores if score > 0]\n",
    "#     top_word_scores = sorted(filtered_word_scores, key=lambda x: x[1], reverse=True)[:1]\n",
    "#     for word, score in top_word_scores:\n",
    "#         print(f\"{word}: {score}\")\n",
    "#         key_words.append(word)\n",
    "#     # print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# key_words = list(set(key_words))\n",
    "# len(key_words)\n",
    "# 将key_words[500:]写入txt文件\n",
    "# with open('./output/sentiment/key_words.txt', 'w', encoding='utf-8') as f:\n",
    "#     for word in key_words[500:]:\n",
    "#         f.write(word + '\\n')\n",
    "# 根据给定的情感唤醒度分类逻辑和互联网语言特点进行词汇归类\n",
    "\n",
    "# low = ['惊报', '低烧', '发癫', '抓心', '惆怅', '浪费', '副作用',  '消停', '好使', '逼逼', '不见得', '差不离', '反胃', '杜撰', '是吗', '殆尽', '求求', '担心', '难听','善忘']\n",
    "# high = ['封杀', '丧心病狂', 'die','谎言', '骗人', '砖家', '灭霸', '混乱', '争论', '痛苦', '夸张', '不行']\n",
    "# middle = ['适可而止','免得', '保持', '商量', '安全',  '运气', '友好', '团结', '争论']\n",
    "\n",
    "# # 将这三类添加到low_sentiment.txt, high_sentiment.txt, middle_sentiment.txt中\n",
    "# with open('./output/sentiment/high_sentiment_seed.txt', 'a', encoding='utf-8') as f:\n",
    "#     for word in high:\n",
    "#         f.write(word + '\\n')\n",
    "\n",
    "# with open('./output/sentiment/low_sentiment_seed.txt', 'a', encoding='utf-8') as f:\n",
    "#     for word in low:\n",
    "#         f.write(word + '\\n')\n",
    "\n",
    "# with open('./output/sentiment/middle_sentiment_seed.txt', 'a', encoding='utf-8') as f:\n",
    "#     for word in middle:\n",
    "#         f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情感评分计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lexicons\n",
    "with open('./output/sentiment/high_sentiment_full.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    high = [line for line in lines if line]\n",
    "\n",
    "with open('./output/sentiment/low_sentiment_full.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    low = [line for line in lines if line]\n",
    "\n",
    "with open('./output/sentiment/middle_sentiment_full.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines]\n",
    "    middle = [line for line in lines if line]\n",
    "\n",
    "\n",
    "high_emotion_words = high\n",
    "middle_emotion_words = middle\n",
    "low_emotion_words = low\n",
    "\n",
    "words = high + low + middle\n",
    "words = list(set(words))\n",
    "\n",
    "# add words to ltp\n",
    "ltp.add_words(words)\n",
    "\n",
    "\n",
    "\n",
    "negative_words = [\n",
    "    '没敢', '不是', '不', '没', '无', '非', '莫', '弗', '毋',\n",
    "    '勿', '未', '否', '别', '休', '無', '不曾', '未必', '没有',\n",
    "    '不要', '难以', '未曾', '并非', '绝不', '不可'\n",
    "]\n",
    "\n",
    "# Dictionary of emotional words and their associated sentiment types\n",
    "all_emotion_words = {**{word: 'high' for word in high_emotion_words},\n",
    "                     **{word: 'middle' for word in middle_emotion_words},\n",
    "                     **{word: 'low' for word in low_emotion_words}}\n",
    "\n",
    "\n",
    "# Function to analyze the sentiment of a sentence\n",
    "def analyze_sentiment(sentence):\n",
    "    # Process the sentence using the LTP pipeline to obtain words and dependencies\n",
    "    output = ltp.pipeline([sentence], tasks=[\"cws\", \"dep\"])\n",
    "    words = output['cws'][0]\n",
    "    dependencies = output['dep'][0]  # Directly get the result of dependency parsing\n",
    "\n",
    "    # Initialize emotional intensity counters\n",
    "    emotion_intensity = {'high': 0, 'middle': 0, 'low': 0}\n",
    "\n",
    "    # Iterate through each word and its dependency relations\n",
    "    for i, (word, head_index, relation) in enumerate(zip(words, dependencies['head'], dependencies['label'])):\n",
    "        head_word = words[head_index - 1] if head_index > 0 else 'ROOT'  # LTP indexing starts from 1\n",
    "\n",
    "        # Calculate the number of negations affecting the sentiment words\n",
    "        negation_count = sum(1 for w in words[:i] if w in negative_words)\n",
    "\n",
    "        if word in all_emotion_words:\n",
    "            emotion_type = all_emotion_words[word]\n",
    "            adjusted_intensity = calculate_intensity_adjustment(negation_count)\n",
    "            emotion_intensity[emotion_type] += adjusted_intensity\n",
    "\n",
    "    # Determine the sentiment type based on the highest emotional intensity\n",
    "    if len(set(emotion_intensity.values())) <= 1:\n",
    "        return 'None', 0\n",
    "\n",
    "    sentiment_type = max(emotion_intensity, key=emotion_intensity.get)\n",
    "    return sentiment_type, emotion_intensity[sentiment_type]\n",
    "\n",
    "\n",
    "# Function to calculate the adjustment in intensity based on negations\n",
    "def calculate_intensity_adjustment(negation_count):\n",
    "    # Formula to adjust emotional intensity based on the presence of negation words\n",
    "    adjusted_intensity = (-1)**negation_count * 0.5 * (1 ** 0.5)  # Assuming the initial intensity of an emotion word is 1\n",
    "    return adjusted_intensity\n",
    "\n",
    "\n",
    "\n",
    "# Function to preprocess text by removing HTML tags, hyperlinks, user mentions, hashtags,\n",
    "# special characters, and specific unwanted patterns\n",
    "def preprocess_text(text):\n",
    "    text = emoji.demojize(text, language='zh')  # Convert emojis to text\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)  # Remove hyperlinks\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove user mentions\n",
    "    text = re.sub(r'#\\S+#', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^\\w\\s，。！？、:]+', '', text)  # Remove special characters but keep Chinese characters and punctuation\n",
    "    text = re.sub(r'转发原文抱歉，作者已设置仅展示半年内微博，此微博已不可见。', '', text)  # Remove specific unwanted text\n",
    "    text = re.sub(r'6', '无语', text)  # Replace number 6 with the word for 'speechless'\n",
    "    text = re.sub(r'\\s2\\S+$', '', text)  # Remove trailing addresses starting with '2'\n",
    "    text = re.sub(r'母羊|公羊', '感染', text)  # Replace 'ewe' or 'ram' with 'infection'\n",
    "    \n",
    "    return text.strip()  # Trim whitespaces from the beginning and the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the ordinary people corpse\n",
    "data = pd.read_csv('./output/sentiment/o_content_info.csv')\n",
    "# data['content'].to_list()\n",
    "data['cl_content'] = data['content'].apply(preprocess_text)\n",
    "data = data[data['cl_content'] != '']\n",
    "data = data[data['cl_content'] != '转发微博']\n",
    "data = data.loc[~(data['cl_content'].apply(lambda x: True if '轻症都有盲盒呀，而且年轻人是大多数' in x else False))]\n",
    "data = data.loc[~(data['cl_content'].apply(lambda x: True if '比比皆是，譬如甲流中招又发烧的就是警示，建议煎几副加减八味肾气汤加味' in x else False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# def process_batch(content_batch):\n",
    "#     \"\"\"处理一个数据批次，并返回情感和强度列表\"\"\"\n",
    "#     batch_sentiment = []\n",
    "#     batch_intensity = []\n",
    "#     for sentence in content_batch:\n",
    "#         sent, intens = analyze_sentiment(sentence)\n",
    "#         batch_sentiment.append(sent)\n",
    "#         batch_intensity.append(intens)\n",
    "#     return batch_sentiment, batch_intensity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is a DataFrame and 'cl_content' is the column with text data\n",
    "content = data['cl_content'].tolist()\n",
    "\n",
    "# Replace full-width space character with a regular space\n",
    "processed_content = [sentence.replace('\\u3000', ' ') for sentence in content]\n",
    "\n",
    "# Initialize lists to store sentiment analysis results\n",
    "sentiments = []\n",
    "intensity = []\n",
    "\n",
    "# Analyze sentiment for each sentence in the content list\n",
    "for sentence in tqdm(processed_content, desc=\"Analyzing Sentiments\"):\n",
    "    try:\n",
    "        sent, intens = analyze_sentiment(sentence)\n",
    "        sentiments.append(sent)\n",
    "        intensity.append(intens)\n",
    "    except Exception as e:\n",
    "        # Handle exceptions (e.g., errors in sentiment analysis)\n",
    "        print(f\"Error processing sentence: {sentence[:50]}... Error: {e}\")\n",
    "        sentiments.append('None')\n",
    "        intensity.append(0)\n",
    "\n",
    "# Add sentiment and intensity results to the original DataFrame\n",
    "data['sentiment'] = sentiments\n",
    "data['intensity'] = intensity\n",
    "\n",
    "# Create a copy of the DataFrame for further modifications\n",
    "data_copy = data.copy()\n",
    "\n",
    "# Post-process to refine sentiment classification based on specific content cues\n",
    "for idx, (content, sentiment) in enumerate(zip(data_copy['cl_content'], data_copy['sentiment'])):\n",
    "    if sentiment == 'None' and '？' in content:\n",
    "        data_copy.at[idx, 'sentiment'] = 'low'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_sum = 1383 + 1037 + 677\n",
    "# high_p = 1383 / s_sum\n",
    "# middle_p = 1037 / s_sum\n",
    "# low_p = 677 / s_sum\n",
    "# print(high_p, middle_p, low_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy.to_csv('./output/sentiment/data_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk appeal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mainstream media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read risk-related words from a text file and preprocess them\n",
    "def read_risk_words(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        processed_lines = [line.strip() for line in lines if line.strip()]\n",
    "    return processed_lines\n",
    "\n",
    "# Load risk and no-risk words\n",
    "risk_words = read_risk_words('./output/risk_media/risk_words.txt')\n",
    "no_risk_words = read_risk_words('./output/risk_media/norisk_words.txt')\n",
    "\n",
    "# Combine and deduplicate the words\n",
    "all_risk_related_words = list(set(risk_words + no_risk_words))\n",
    "\n",
    "# Add words to the LTP's vocabulary\n",
    "ltp.add_words(all_risk_related_words)\n",
    "\n",
    "# Create a dictionary to classify words as 'yes' for risk or 'no' for no-risk\n",
    "all_risk_words = {**{word: 'yes' for word in risk_words}, **{word: 'no' for word in no_risk_words}}\n",
    "\n",
    "# List of negation words that might alter the context of risk words\n",
    "negative_words = [\n",
    "    '没敢', '不是', '不', '没', '无', '非', '莫', '弗', '毋',\n",
    "    '勿', '未', '否', '别', '休', '无', '不曾', '未必', '没有',\n",
    "    '不要', '难以', '未曾', '并非', '绝不', '不可'\n",
    "]\n",
    "\n",
    "# Function to analyze the risk in a sentence using LTP\n",
    "def analyze_risk(sentence):\n",
    "    # Process the sentence with LTP for word segmentation and dependency parsing\n",
    "    output = ltp.pipeline([sentence], tasks=[\"cws\", \"dep\"])\n",
    "    words = output['cws'][0]\n",
    "    dependencies = output['dep'][0]\n",
    "\n",
    "    risk_intensity = {'yes': 0, 'no': 0}\n",
    "\n",
    "    # Analyze each word and its dependencies to assess risk\n",
    "    for i, (word, head_index, relation) in enumerate(zip(words, dependencies['head'], dependencies['label'])):\n",
    "        head_word = words[head_index - 1] if head_index > 0 else 'ROOT'  # LTP's head indices start from 1\n",
    "\n",
    "        # Check if a negation word directly modifies a risk word\n",
    "        negation_count = sum(1 for w in words[:i] if w in negative_words and words[dependencies['head'][i] - 1] in all_risk_words)\n",
    "\n",
    "        if word in all_risk_words:\n",
    "            risk_type = all_risk_words[word]\n",
    "            adjusted_intensity = calculate_intensity_adjustment(negation_count)\n",
    "            risk_intensity[risk_type] += adjusted_intensity\n",
    "\n",
    "    # Determine the predominant risk category\n",
    "    if len(set(risk_intensity.values())) <= 1:\n",
    "        return 'None', 0  # If all intensities are equal or zero, categorize as 'None'\n",
    "\n",
    "    risk_type = max(risk_intensity, key=risk_intensity.get)\n",
    "    return risk_type, risk_intensity[risk_type]\n",
    "\n",
    "# Function to adjust intensity based on the presence of negation words\n",
    "def calculate_intensity_adjustment(negation_count):\n",
    "    # Adjust the intensity based on negation; example formula below\n",
    "    return (-1) ** negation_count * 0.5 * (1 ** 0.5)  # Assuming initial intensity is 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_content_info = pd.read_csv('./output/risk_media/m_content_info.csv')\n",
    "# media = data[~(data['verify_typ'] == '没有认证')]\n",
    "m_content_info['cl_content'] = m_content_info['content'].apply(lambda x: preprocess_text(x))\n",
    "m_content_info = m_content_info[m_content_info['cl_content'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = m_content_info.copy()\n",
    "data['cl_content'] = data['cl_content'].apply(lambda x: str(x))\n",
    "data = data[~(data['cl_content'].apply(lambda x: True if '并不是所有的口干都是上火' in x else False))]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "no_index = []\n",
    "yes_index = []\n",
    "yes_index.extend(list(data[(data['cl_content'].apply(lambda x: True if '转发原文七成半新冠康复者受后遗症困扰' in x else False))].index))\n",
    "yes_index.extend(list(data[(data['cl_content'].apply(lambda x: True if '症状可持续数周、数月甚至更长时间10月13日' in x else False))].index))\n",
    "content = data['cl_content'].tolist()\n",
    "\n",
    "risks = []\n",
    "intensity = []\n",
    "\n",
    "for i, sentence in tqdm(enumerate(content), desc=\"分析风险\", total=len(content)):\n",
    "    if i not in yes_index:\n",
    "        risk, intens = analyze_risk(sentence)\n",
    "        risks.append(risk)\n",
    "        intensity.append(intens)\n",
    "    else:\n",
    "        risks.append('yes')\n",
    "        intensity.append(0)\n",
    "for id in yes_index:\n",
    "    risks[id] = 'yes'\n",
    "    intensity[id] = 0\n",
    "data['risk'] = risks\n",
    "data.loc[data['content'].apply(lambda x: True if '崇雨田' in x else False), 'risk'] = 'no'\n",
    "data.loc[data['content'].apply(lambda x: True if '梁连春' in x else False), 'risk'] = 'no'\n",
    "data.loc[data['content'].apply(lambda x: True if '美国研究' in x else False), 'risk'] = 'yes'\n",
    "data.loc[data['content'].apply(lambda x: True if '牛津大学' in x else False), 'risk'] = 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./output/risk_media/data_risk_mainstream.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./output/risk_media/w_content_info.csv')\n",
    "data['cl_content'] = data['content'].apply(lambda x: str(x))\n",
    "data = data[~(data['cl_content'].apply(lambda x: True if '并不是所有的口干都是上火' in x else False))]\n",
    "data['cl_content'] = data['cl_content'].apply(lambda x: preprocess_text(x))\n",
    "data = data[data['cl_content'] != '']\n",
    "data = data.reset_index(drop=True)\n",
    "no_index = []\n",
    "yes_index = []\n",
    "yes_index.extend(list(data[(data['cl_content'].apply(lambda x: True if '中国疾控中心流行病学首席专家吴尊友表示' in x else False))].index))\n",
    "yes_index.extend(list(data[(data['cl_content'].apply(lambda x: True if '转发原文七成半新冠康复者受后遗症困扰' in x else False))].index))\n",
    "yes_index.extend(list(data[(data['cl_content'].apply(lambda x: True if '症状可持续数周、数月甚至更长时间10月13日' in x else False))].index))\n",
    "yes_index.extend(list(data[(data['cl_content'].apply(lambda x: True if '在国务院联防联控机制新闻发布会上，中国疾控中心流行病学首席专家' in x else False))].index))\n",
    "yes_index.extend(list(data[(data['cl_content'].apply(lambda x: True if '吴尊友' in x else False))].index))\n",
    "yes_index = list(set(yes_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content = data['cl_content'].tolist()\n",
    "\n",
    "risks = []\n",
    "intensity = []\n",
    "\n",
    "for i, sentence in tqdm(enumerate(content), desc=\"分析风险\", total=len(content)):\n",
    "    if i not in yes_index:\n",
    "        risk, intens = analyze_risk(sentence)\n",
    "        risks.append(risk)\n",
    "        intensity.append(intens)\n",
    "    else:\n",
    "        risks.append('yes')\n",
    "        intensity.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['risk'] = risks\n",
    "data['risk'].value_counts()\n",
    "data.loc[data['content'].apply(lambda x: True if '崇雨田' in str(x) else False), 'risk'] = 'no'\n",
    "data.loc[data['content'].apply(lambda x: True if '梁连春' in str(x) else False), 'risk'] = 'no'\n",
    "data.loc[data['content'].apply(lambda x: True if '美国研究' in str(x) else False), 'risk'] = 'yes'\n",
    "data.loc[data['content'].apply(lambda x: True if '牛津大学' in str(x) else False), 'risk'] = 'yes'\n",
    "\n",
    "\n",
    "data.loc[data['content'].apply(lambda x: True if '能有什么后遗症' in str(x) else False), 'risk'] = 'no'\n",
    "data.loc[data['content'].apply(lambda x: True if '白肺患者临床' in str(x) else False), 'risk'] = 'no'\n",
    "data.loc[data['content'].apply(lambda x: True if '都会有后遗症' in str(x) else False), 'risk'] = 'yes'\n",
    "data.loc[data['content'].apply(lambda x: True if '心率不齐' in str(x) else False), 'risk'] = 'yes'\n",
    "data.loc[data['content'].apply(lambda x: True if '非高危组重症为0' in str(x) else False), 'risk'] = 'yes'\n",
    "\n",
    "data['risk'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('./output/risk_media/data_risk_wemedia.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the empirical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentiment = pd.read_csv('./output/sentiment/data_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(data_sentiment[data_sentiment['sentiment'].isna()]['cl_content'].value_counts().index)\n",
    "# with open('./sentiment_na_content_cl.txt', 'w', encoding='utf-8') as f:\n",
    "#     for item in list(data_sentiment[data_sentiment['sentiment'].isna()]['cl_content'].value_counts().index):\n",
    "#         f.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_risk_mainstream = pd.read_csv('./output/risk_media/data_risk_mainstream.csv')\n",
    "missing_dates_count = data_risk_mainstream['date'].isna().sum()\n",
    "print(f\"Number of missing or NaT dates: {missing_dates_count}\")\n",
    "data_risk_wemedia = pd.read_csv('./output/risk_media/data_risk_wemedia.csv')\n",
    "missing_dates_count = data_risk_wemedia['date'].isna().sum()\n",
    "print(f\"Number of missing or NaT dates: {missing_dates_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_risk = ['自限性疾病','成天测','24小时核酸', '能不检就不检']\n",
    "risk = ['做好防护', '持久战','美国经验','新冠长期症状','共存派','经常记不起', \n",
    "        '共蠢派', '长期后遗症影响', '焦虑素材', '新冠后遗症受到越来越多的关注', '胸疼', \n",
    "        '阴性对照组有后遗症', '很严重', '也后遗症', '任何后遗症',\n",
    "        '心惊','柳叶刀','新京报']\n",
    "\n",
    "# 更新风险列\n",
    "for index, row in data_risk_wemedia.iterrows():\n",
    "    content = row['cl_content']\n",
    "    # if any(word in str(content) for word in no_risk):\n",
    "    #     data_risk_wemedia.at[index, 'risk'] = 'no'\n",
    "    if any(word in str(content) for word in risk):\n",
    "        data_risk_wemedia.at[index, 'risk'] = 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' column to datetime format\n",
    "data_risk_wemedia['date'] = pd.to_datetime(data_risk_wemedia['date'])\n",
    "\n",
    "# Group the data by 'date' and count occurrences of each 'risk' value\n",
    "data_risk_wemedia = data_risk_wemedia.groupby('date')['risk'].value_counts().unstack().fillna(0)\n",
    "\n",
    "# Add a 'total' column summing up all risk occurrences per day\n",
    "data_risk_wemedia['total'] = data_risk_wemedia.sum(axis=1)\n",
    "\n",
    "# Calculate the proportion of 'yes' and 'no' risk responses\n",
    "data_risk_wemedia['risk_p'] = data_risk_wemedia['yes'] / data_risk_wemedia['total']\n",
    "data_risk_wemedia['norisk_p'] = data_risk_wemedia['no'] / data_risk_wemedia['total']\n",
    "\n",
    "# Reset index to turn the grouped 'date' index back into a column\n",
    "data_risk_wemedia = data_risk_wemedia.reset_index()\n",
    "\n",
    "# Filter data within a specific date range\n",
    "data_risk_wemedia = data_risk_wemedia[\n",
    "    (data_risk_wemedia['date'] >= '2022-10-14') & \n",
    "    (data_risk_wemedia['date'] <= '2023-01-18')\n",
    "]\n",
    "\n",
    "# Calculate the number of days since the start date of the dataset\n",
    "df = data_risk_wemedia.copy()\n",
    "start_date = df['date'].min()\n",
    "df['days_since_start'] = (df['date'] - start_date).dt.days\n",
    "\n",
    "# Create a new period identifier by dividing 'days_since_start' by 7 and taking the floor\n",
    "df['period_id_7d'] = df['days_since_start'] // 7\n",
    "\n",
    "# Group the data by the new 7-day period identifier and aggregate values\n",
    "period_data_7d = df.groupby('period_id_7d').agg(\n",
    "    start_date=('date', 'min'),  # Minimum date in each period\n",
    "    yes_total=('yes', 'sum'),   # Sum of 'yes' labels\n",
    "    no_total=('no', 'sum'),     # Sum of 'no' labels\n",
    "    total_sum=('total', 'sum')  # Sum of all responses\n",
    ").reset_index()\n",
    "\n",
    "# Calculate the percentage of risk and no-risk responses per period\n",
    "period_data_7d['risk_p'] = period_data_7d['yes_total'] / period_data_7d['total_sum']\n",
    "period_data_7d['norisk_p'] = period_data_7d['no_total'] / period_data_7d['total_sum']\n",
    "\n",
    "# Save the aggregated data to a CSV file\n",
    "period_data_7d.to_csv('./output/empirical/w_media_risk_data_7days.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_risk_mainstream['date'] = pd.to_datetime(data_risk_mainstream['date'])\n",
    "data_risk_mainstream['date'] = pd.to_datetime(data_risk_mainstream['date'])\n",
    "date_risk_mainstream =  data_risk_mainstream.groupby('date')['risk'].value_counts().unstack().fillna(0)\n",
    "date_risk_mainstream['total'] = date_risk_mainstream.sum(axis=1)\n",
    "\n",
    "# date_risk_mainstream.to_csv('./date_risk_mainstream.csv')\n",
    "date_risk_mainstream['risk_p'] = date_risk_mainstream['yes'] / date_risk_mainstream['total']\n",
    "date_risk_mainstream['norisk_p'] = date_risk_mainstream['no'] / date_risk_mainstream['total']\n",
    "date_risk_mainstream = date_risk_mainstream.reset_index()\n",
    "date_risk_mainstream = date_risk_mainstream[(date_risk_mainstream['date'] >= '2022-10-14') & (date_risk_mainstream['date'] <= '2023-01-18')]\n",
    "date_risk_mainstream\n",
    "\n",
    "df = date_risk_mainstream.copy()\n",
    "start_date = df['date'].min()\n",
    "df['days_since_start'] = (df['date'] - start_date).dt.days\n",
    "\n",
    "\n",
    "df['period_id_7d'] = df['days_since_start'] // 7\n",
    "\n",
    "\n",
    "period_data_7d = df.groupby('period_id_7d').agg(\n",
    "    start_date=('date', 'min'),  \n",
    "    yes_total=('yes', 'sum'),\n",
    "    no_total=('no', 'sum'),\n",
    "    # middle_total=('middle', 'sum'),\n",
    "    total_sum=('total', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "period_data_7d['risk_p'] = period_data_7d['yes_total'] / period_data_7d['total_sum']\n",
    "period_data_7d['norisk_p'] = period_data_7d['no_total'] / period_data_7d['total_sum']\n",
    "\n",
    "period_data_7d.to_csv('./output/empirical/m_media_risk_data_7days.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentiment = pd.read_csv('./output/sentiment/data_sentiment.csv')\n",
    "middle_words = [\n",
    " \"戒烟戒酒\", \"实事求是\",\n",
    "    '你信就是了','矫枉过正','比以前棒','会好点','最有价值','很有水平','不矛盾','还好', '选择信人品', '阿航', '照顾好自己', '多喝热水', '大实话'\n",
    "]\n",
    "high_words = ['他们都骂']\n",
    "low_words = ['我怕', '大明白', '啥都不懂', '很无语']\n",
    "\n",
    "for index, row in data_sentiment.iterrows():\n",
    "    content = row['cl_content']\n",
    "    if any(word in content for word in middle_words):\n",
    "        data_sentiment.at[index, 'sentiment'] = 'middle'\n",
    "    elif any(word in content for word in high_words):\n",
    "        data_sentiment.at[index, 'sentiment'] = 'high'\n",
    "    elif any(word in content for word in low_words):\n",
    "        data_sentiment.at[index, 'sentiment'] = 'low'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sentiment['date'] = pd.to_datetime(data_sentiment['date'])\n",
    "data_sentiment.groupby('date')['sentiment'].value_counts().unstack().fillna(0)\n",
    "\n",
    "date_sentiment = data_sentiment.groupby('date')['sentiment'].value_counts().unstack().fillna(0)\n",
    "date_sentiment['total'] = date_sentiment.sum(axis=1)\n",
    "date_sentiment['high_p'] = date_sentiment['high'] / date_sentiment['total']\n",
    "date_sentiment['middle_p'] = date_sentiment['middle'] / date_sentiment['total']\n",
    "date_sentiment['low_p'] = date_sentiment['low'] / date_sentiment['total']\n",
    "date_sentiment = date_sentiment.reset_index()\n",
    "date_sentiment_sample = date_sentiment[(date_sentiment['date'] >= '2022-10-14') & (date_sentiment['date'] <= '2023-01-18')]\n",
    "date_sentiment_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = date_sentiment_sample.copy()\n",
    "start_date = df['date'].min()\n",
    "df['days_since_start'] = (df['date'] - start_date).dt.days\n",
    "\n",
    "\n",
    "df['period_id_3d'] = df['days_since_start'] // 7\n",
    "\n",
    "\n",
    "period_data_7d = df.groupby('period_id_3d').agg(\n",
    "    start_date=('date', 'min'),  \n",
    "    high_total=('high', 'sum'),\n",
    "    low_total=('low', 'sum'),\n",
    "    middle_total=('middle', 'sum'),\n",
    "    total_sum=('total', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "period_data_7d['high_p'] = period_data_7d['high_total'] / period_data_7d['total_sum']\n",
    "period_data_7d['middle_p'] = period_data_7d['middle_total'] / period_data_7d['total_sum']\n",
    "period_data_7d['low_p'] = period_data_7d['low_total'] / period_data_7d['total_sum']\n",
    "\n",
    "\n",
    "period_data_7d.to_csv('./output/empirical/sentiment_data_7days.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 绘制两幅图，分别是high_p, middle_p, low_p的时间序列图和high, middle, low的时间序列图\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# period_data_7d.plot(x='period_id_7d', y=['high_p', 'middle_p', 'low_p'], ax=ax[0], title='Sentiment Proportion Over Time', color=['r', 'g', 'b'], linestyle='-', linewidth=0.5)\n",
    "# # period_data_3d.plot(x='period_id_3d', y=['high_total', 'middle_total', 'low_total'], ax=ax[1], title='Sentiment Count Over Time', color=['r', 'g', 'b'],  linestyle='-', linewidth=0.5)\n",
    "\n",
    "# plt.show()\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # 假设的平滑窗口大小\n",
    "# window_size = 3\n",
    "\n",
    "# # 计算滚动平均以平滑数据\n",
    "# period_data_7d['high_p_smooth'] = period_data_7d['high_p'].rolling(window=window_size, min_periods=1).mean()\n",
    "# period_data_7d['middle_p_smooth'] = period_data_7d['middle_p'].rolling(window=window_size, min_periods=1).mean()\n",
    "# period_data_7d['low_p_smooth'] = period_data_7d['low_p'].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "# # 绘制子图\n",
    "# fig, axs = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
    "\n",
    "# # 配置标题和轴标签\n",
    "# titles = ['High Sentiment Proportion', 'Middle Sentiment Proportion', 'Low Sentiment Proportion']\n",
    "# colors = ['red', 'green', 'blue']\n",
    "# smooth_labels = ['high_p_smooth', 'middle_p_smooth', 'low_p_smooth']\n",
    "# original_labels = ['high_p', 'middle_p', 'low_p']\n",
    "\n",
    "# for i, ax in enumerate(axs):\n",
    "#     period_data_7d.plot(x='period_id_3d', y=original_labels[i], ax=ax, linestyle='-', color=colors[i], alpha=0.5, linewidth=1, label='Original')\n",
    "#     period_data_7d.plot(x='period_id_3d', y=smooth_labels[i], ax=ax, linestyle='-', color=colors[i], linewidth=2, label='Smoothed')\n",
    "#     ax.set_title(titles[i])\n",
    "#     ax.set_xlabel('Period')\n",
    "#     ax.set_ylabel('Proportion')\n",
    "#     ax.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ltp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
